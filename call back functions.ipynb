{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bffb79d-1c15-4c7d-8c56-1d02464677f1",
   "metadata": {},
   "source": [
    "## Q1. Install and load the latest versions of TensorFlow and Keras. Print their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b90d2-f790-4f51-8a91-ef22eec5b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08866ace-8554-4c27-8e09-cedea5f5c501",
   "metadata": {},
   "source": [
    "## Q2. Load the Wine Quality dataset and explore its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ea38a1-1f19-4e98-a843-3605b035f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da0535-68ec-49b3-be27-5f69b7686b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = pd.read_csv(\"winequality.csv\")\n",
    "\n",
    "# Explore the dimensions of the dataset\n",
    "print(\"Number of rows:\", len(dataset))\n",
    "print(\"Number of columns:\", len(dataset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1921632-3a96-41f0-a6de-a8c2662343ba",
   "metadata": {},
   "source": [
    "## Q3. Check for null values, identify categorical variables, and encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a9165-2a0d-4276-8292-14cd7a30b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "To check for null values, identify categorical variables, and encode them in the Wine Quality dataset, you can follow these\n",
    "steps. I'll use Python and the pandas library for data manipulation:\n",
    "\n",
    "1.Check for Null Values:\n",
    "\n",
    "You can check for null values in the dataset using the isnull() function and then sum the null values for each column.\n",
    "\n",
    "\n",
    "# Check for null values in the Red Wine dataset\n",
    "red_wine_null_values = red_wine_data.isnull().sum()\n",
    "print(\"Null values in Red Wine dataset:\")\n",
    "print(red_wine_null_values)\n",
    "\n",
    "# Check for null values in the White Wine dataset (if applicable)\n",
    "white_wine_null_values = white_wine_data.isnull().sum()\n",
    "print(\"\\nNull values in White Wine dataset:\")\n",
    "print(white_wine_null_values)\n",
    "\n",
    "\n",
    "1.Identify Categorical Variables:\n",
    "\n",
    "Categorical variables are typically non-numeric columns that represent categories or labels. In the Wine Quality dataset, \n",
    "you may consider the \"quality\" column as a categorical variable.\n",
    "\n",
    "# Identify categorical variables (e.g., 'quality' column)\n",
    "categorical_columns = ['quality']\n",
    "\n",
    "# For the Red Wine dataset, you can also consider 'type' as a categorical variable\n",
    "# categorical_columns_red = ['quality', 'type']\n",
    "\n",
    "# For the White Wine dataset (if applicable)\n",
    "# categorical_columns_white = ['quality', 'type']\n",
    "\n",
    "\n",
    "1.Encode Categorical Variables:\n",
    "\n",
    "To work with categorical variables, you can encode them using techniques like one-hot encoding or label encoding. In this\n",
    "example, I'll use label encoding, which assigns a unique integer to each category.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'quality' column in the Red Wine dataset\n",
    "red_wine_data['quality_encoded'] = label_encoder.fit_transform(red_wine_data['quality'])\n",
    "\n",
    "# For the White Wine dataset (if applicable)\n",
    "# white_wine_data['quality_encoded'] = label_encoder.fit_transform(white_wine_data['quality'])\n",
    "\n",
    "# Drop the original 'quality' column (if needed)\n",
    "red_wine_data = red_wine_data.drop('quality', axis=1)\n",
    "\n",
    "# For the White Wine dataset (if applicable)\n",
    "# white_wine_data = white_wine_data.drop('quality', axis=1)\n",
    "\n",
    "Now, the \"quality\" column has been encoded with numerical values. You can repeat the encoding process for other categorical \n",
    "columns if they exist.\n",
    "\n",
    "Keep in mind that for more advanced encoding techniques or handling missing values, you might need to use more complex\n",
    "methods, such as one-hot encoding for nominal categorical variables or imputation for missing values. The approach described\n",
    "above is a basic way to handle these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c80f7-5876-499f-bfaa-091550b6b259",
   "metadata": {},
   "source": [
    "## Q4. Separate the features and target variables from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc443dd-1fc5-4941-8c79-6e056b0e8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable for the Red Wine dataset\n",
    "red_wine_features = red_wine_data.drop('quality_encoded', axis=1)\n",
    "red_wine_target = red_wine_data['quality_encoded']\n",
    "\n",
    "# For the White Wine dataset (if applicable)\n",
    "# white_wine_features = white_wine_data.drop('quality_encoded', axis=1)\n",
    "# white_wine_target = white_wine_data['quality_encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d435a2c-64c9-499c-abf2-b9a80f2b64c6",
   "metadata": {},
   "source": [
    "## Q5. Perform a train-test split and divide the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b53b9-1187-4403-8f41-05aefd4acf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training, validation, and test datasets\n",
    "# We'll use an 80-10-10 split (you can adjust these percentages)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(red_wine_features, red_wine_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the temporary data into validation and test datasets (50% each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fdeed6-5e9f-4a5d-b55f-525b8f3fa373",
   "metadata": {},
   "source": [
    "## Q6. Perform scaling on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22edb95-5d67-4c1d-b079-f6487cff9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ed99-7979-4e5d-8055-8489b814bb4f",
   "metadata": {},
   "source": [
    "## Q7. Create at least 2 hidden layers and an output layer for the binary categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8be37c4-617d-4f35-a7c4-921321f96c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tensorboard<2.15,>=2.14\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting keras<2.15,>=2.14.0\n",
      "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting ml-dtypes==0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 keras-2.14.0 libclang-16.0.6 markdown-3.4.4 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 werkzeug-3.0.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67405eb2-5c1a-4641-bb79-5474f86aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer (assumes the number of input features matches the shape of X_train_scaled)\n",
    "model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# First hidden layer (you can adjust the number of units/neurons as needed)\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer with a sigmoid activation function for binary classification\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba0d77-4851-4930-b53c-e82bc783a979",
   "metadata": {},
   "source": [
    "## Q8. Create a Sequential model and add all the layers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33685ac5-5848-4fd8-ac5c-95c85358653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a Sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer (assumes the number of input features matches the shape of X_train_scaled)\n",
    "model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# First hidden layer (you can adjust the number of units/neurons as needed)\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer with a sigmoid activation function for binary classification\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b40c64-4339-4459-b78c-16814cdc6513",
   "metadata": {},
   "source": [
    "## Q9. Implement a TensorBoard callback to visualize and monitor the model's training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414da82-e857-43f6-9547-7e734d3d804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Create a Sequential model and add layers to it (as mentioned earlier)\n",
    "\n",
    "# Set up a TensorBoard callback\n",
    "log_dir = \"logs\"  # Choose a directory to store the logs\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Fit the model with the TensorBoard callback\n",
    "model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_val_scaled, y_val), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9dbe1-bcb0-4989-96aa-e4e62f9fb69b",
   "metadata": {},
   "source": [
    "## Q10. Use Early Stopping to prevent overfitting by monitoring a chosen metric and stopping the training if no improvement is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3e003-270f-4d7c-a14f-55c55f5c0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early stopping is a technique to prevent overfitting by monitoring a chosen metric (typically validation loss or validation \n",
    "accuracy) during training and stopping the training process if no improvement is observed. You can implement early stopping \n",
    "in Keras using the EarlyStopping callback. Here's how to do it:\n",
    "\n",
    "1.Import the necessary libraries, including the EarlyStopping callback:\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "1.Create a Sequential model and add layers to it, similar to the previous examples.\n",
    "\n",
    "2.Set up the EarlyStopping callback by specifying the monitored metric (e.g., validation loss) and conditions for stopping\n",
    "the training. For example, you can monitor the validation loss and specify that training should stop if there is no\n",
    "improvement after a certain number of epochs (e.g., patience=5).\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    ~monitor: The metric to monitor, which is usually the validation loss.\n",
    "    ~patience: The number of epochs with no improvement after which training will be stopped.\n",
    "    ~restore_best_weights: If set to True, the model's weights will be restored to the best weights when training stops.\n",
    "    \n",
    "When fitting your model, include the EarlyStopping callback by passing it to the callbacks argument. Additionally, specify\n",
    "the number of epochs and other training settings.\n",
    "\n",
    "# Fit the model with EarlyStopping callback\n",
    "model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "With the EarlyStopping callback in place, the training process will automatically stop when the chosen metric\n",
    "(validation loss) does not improve for the specified number of epochs. The restore_best_weights option ensures that the\n",
    "model retains the best weights observed during training.\n",
    "\n",
    "Here's the complete code with Early Stopping integrated:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a Sequential model and add layers to it (as mentioned earlier)\n",
    "\n",
    "# Set up an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with EarlyStopping callback\n",
    "model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping_callback])\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a Sequential model and add layers to it (as mentioned earlier)\n",
    "\n",
    "# Set up an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with EarlyStopping callback\n",
    "model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "This approach helps prevent overfitting and ensures that the model stops training when its performance on the validation\n",
    "set no longer improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1a436-0361-419c-933c-31bc2a1186b1",
   "metadata": {},
   "source": [
    "## Q11. Implement a ModelCheckpoint callback to save the best model based on a chosen metric during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af261496-1f35-4b44-a140-5964e5dd87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create a Sequential model and add layers to it (as mentioned earlier)\n",
    "\n",
    "# Set up a ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Path to save the best model checkpoint\n",
    "    monitor='val_loss',       # Metric to monitor (e.g., validation loss)\n",
    "    save_best_only=True,      # Save only the best models\n",
    "    save_weights_only=True,   # Save only the model weights, not the architecture\n",
    "    verbose=1                 # Display a message when a checkpoint is saved\n",
    ")\n",
    "\n",
    "# Fit the model with ModelCheckpoint callback\n",
    "model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39ab15-cb3f-48db-83b6-8ae6376a653d",
   "metadata": {},
   "source": [
    "## Q12. Print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dee2f2-4c0b-4d36-a243-805b27944295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a Sequential model and add layers to it (as shown in previous answers)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a80d5-204d-47aa-8520-440dec5f027c",
   "metadata": {},
   "source": [
    "## Q13. Use binary cross-entropy as the loss function, Adam optimizer, and include the metric ['accuracy']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5495b-ff6f-4da8-a06b-e5763c7e5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    optimizer='adam',            # Adam optimizer\n",
    "    metrics=['accuracy']         # Include accuracy as a metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8160da-ea42-489c-83a7-52268c72f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a Sequential model and add layers to it (as shown in previous answers)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss, Adam optimizer, and accuracy metric\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    optimizer='adam',            # Adam optimizer\n",
    "    metrics=['accuracy']         # Include accuracy as a metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d4cc0-0ecd-4aba-a7ac-da601b250d50",
   "metadata": {},
   "source": [
    "## Q14. Compile the model with the specified loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8858613-845e-4bba-bc31-403b0c256049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a Sequential model and add layers to it (as shown in previous answers)\n",
    "\n",
    "# Compile the model with the specified loss function, optimizer, and metrics\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    optimizer='adam',            # Adam optimizer\n",
    "    metrics=['accuracy']         # Include accuracy as a metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d7a8f-de63-4d9f-aebf-db06921b6a98",
   "metadata": {},
   "source": [
    "## Q15. Fit the model to the data, incorporating the TensorBoard, Early Stopping, and ModelCheckpoint callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4ad23-0b6b-4b3f-bca3-6b93dece9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create a Sequential model and add layers to it\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with specified settings\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    optimizer='adam',            # Adam optimizer\n",
    "    metrics=['accuracy']         # Include accuracy as a metric\n",
    ")\n",
    "\n",
    "# Set up the callbacks\n",
    "log_dir = \"logs\"  # TensorBoard log directory\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',   # Metric to monitor (validation loss)\n",
    "    patience=5,           # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Path to save the best model checkpoint\n",
    "    monitor='val_loss',       # Metric to monitor (validation loss)\n",
    "    save_best_only=True,      # Save only the best models\n",
    "    save_weights_only=True    # Save only the model weights, not the architecture\n",
    ")\n",
    "\n",
    "# Fit the model with the specified callbacks\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52eb47-4e82-45ab-bc0d-3e6fa808d817",
   "metadata": {},
   "source": [
    "## Q16. Get the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f39cf-29c4-4b10-8ed5-055a2c92445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's parameters (weights and biases)\n",
    "model_params = model.get_weights()\n",
    "\n",
    "# Access and inspect the parameters for each layer\n",
    "for i, layer_params in enumerate(model_params):\n",
    "    print(f\"Layer {i} parameters:\")\n",
    "    print(layer_params)\n",
    "    print(\"Shape:\", layer_params.shape)\n",
    "    print()\n",
    "\n",
    "# Total number of parameters in the model\n",
    "total_params = sum(layer_params.size for layer_params in model_params)\n",
    "print(\"Total number of parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e1fa6-2444-4803-878c-1fece18d9492",
   "metadata": {},
   "source": [
    "## Q17. Store the model's training history as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89361b-8f59-474a-998a-11f857eebd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you've trained your model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Create a Pandas DataFrame from the training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Display the training history\n",
    "print(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea084a2-0247-49fa-b11b-886394fefddf",
   "metadata": {},
   "source": [
    "## Q18. Plot the model's training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b302e-fd7e-4722-8c5d-e08c0b627b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've trained your model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Create plots for training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Create plots for training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e063fe-6518-40f4-85a2-dab81d92fc27",
   "metadata": {},
   "source": [
    "## Q19. Evaluate the model's performance using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90b66a-8f6d-4088-a9b3-0864c0ca00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've trained your model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
